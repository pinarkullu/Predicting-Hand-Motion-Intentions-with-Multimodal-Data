# -*- coding: utf-8 -*-
"""Untitled150.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mrqjzoPEuK2HsiDXZt3QMqKo-1mBzEIV

Logistic Regression
"""

import pandas as pd
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score
from sklearn.preprocessing import StandardScaler


train_data = pd.read_csv("")
test_data = pd.read_csv("")

train_data.drop("Direction", axis = 1, inplace = True)
test_data.drop("Direction", axis = 1, inplace = True)

x_train = train_data.drop("label", axis = 1)
x_test = test_data.drop("label", axis = 1)
y_train = train_data["label"]
y_test = test_data["label"]

scale = StandardScaler()
x_train = scale.fit_transform(x_train)
x_test = scale.fit_transform(x_test)

model = LogisticRegression(multi_class='ovr')
model.fit(x_train, y_train)

# Make predictions
y_pred = model.predict(x_test)

# Calculate performance metrics
accuracy = accuracy_score(y_test, y_pred)
recall = recall_score(y_test, y_pred, average='weighted')
precision = precision_score(y_test, y_pred, average='weighted')
f1 = f1_score(y_test, y_pred, average='weighted')

# Print the performance metrics
print(f"Accuracy: {accuracy:.4f}")
print(f"Recall: {recall:.4f}")
print(f"Precision: {precision:.4f}")
print(f"F1 Score: {f1:.4f}")

"""KNN"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, f1_score, recall_score
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
import matplotlib.pyplot as plt


train_data = pd.read_csv("/content/circle_train_data.csv")
test_data = pd.read_csv("/content/test_circle_file.csv")

train_data.drop("Direction", axis = 1, inplace = True)
test_data.drop("Direction", axis = 1, inplace = True)

x_train = train_data.drop("label", axis = 1)
x_test = test_data.drop("label", axis = 1)
y_train = train_data["label"]
y_test = test_data["label"]

scale = StandardScaler()
x_train = scale.fit_transform(x_train)
x_test = scale.fit_transform(x_test)

# Define a range of K values to experiment with
k_values = 3  # You can adjust this range

# Create lists to store performance metrics
test_accuracy_scores = []
test_f1_scores = []
test_recall_scores = []

# Iterate over different K values
knn_classifier = KNeighborsClassifier(n_neighbors=k_value)

knn_classifier.fit(x_train, y_train)

y_pred = knn_classifier.predict(x_test)

test_accuracy = accuracy_score(y_test, y_pred)
test_f1 = f1_score(y_test, y_pred, average='weighted')
test_recall = recall_score(y_test, y_pred, average='weighted')

test_accuracy_scores.append(test_accuracy)
test_f1_scores.append(test_f1)
test_recall_scores.append(test_recall)

"""SVM

"""

import pandas as pd
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score
from sklearn.preprocessing import StandardScaler

train_data = pd.read_csv("/content/circle_train_data.csv")
test_data = pd.read_csv("/content/test_circle_file.csv")


train_data.drop("Direction", axis = 1, inplace = True)
test_data.drop("Direction", axis = 1, inplace = True)

x_train = train_data.drop("label", axis = 1)
x_test = test_data.drop("label", axis = 1)
y_train = train_data["label"]
y_test = test_data["label"]

scale = StandardScaler()

x_train = scale.fit_transform(x_train)
x_test = scale.fit_transform(x_test)

from sklearn.model_selection import GridSearchCV
from sklearn.svm import SVC

# Define the parameter grid to search
param_grid = {
    'C': [0.1, 1, 10],
    'kernel': ['linear', 'rbf', 'poly'],
}

# Create an SVM classifier
svm_classifier = SVC()

# Create a grid search object with cross-validation
grid_search = GridSearchCV(svm_classifier, param_grid, cv=5, scoring='f1_weighted')

# Fit the grid search to the data
grid_search.fit(x_train, y_train)

# Get the best hyperparameters
best_params = grid_search.best_params_
print("Best Hyperparameters:", best_params)

# Get the best estimator (trained model)
best_svm_model = grid_search.best_estimator_

# Evaluate the best model on the test set
y_pred = best_svm_model.predict(x_test)

# Calculate accuracy, precision, recall, and F1-score
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred, average='weighted')
recall = recall_score(y_test, y_pred, average='weighted')
f1 = f1_score(y_test, y_pred, average='weighted')

print(f"Accuracy: {accuracy}")
print(f"Precision: {precision}")
print(f"Recall: {recall}")
print(f"F1-score: {f1}")

"""NN"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix

train_data = pd.read_csv("")
test_data = pd.read_csv("")


train_data.drop("Direction", axis = 1, inplace = True)
test_data.drop("Direction", axis = 1, inplace = True)
train_data.drop(["UserId", "Date", "Time"], axis = 1, inplace = True)
test_data.drop(["UserId", "Date", "Time"], axis = 1, inplace = True)

x_train = train_data.drop("label", axis = 1)
x_test = test_data.drop("label", axis = 1)
y_train = train_data["label"] -1
y_test = test_data["label"] -1


scale = StandardScaler()

x_train = scale.fit_transform(x_train)
x_test = scale.fit_transform(x_test)


# Define a neural network model
model = keras.Sequential([
    layers.Input(shape=(24,)),
    layers.Dense(64, activation='relu'),  # Hidden layer with 64 neurons and ReLU activation
    layers.Dense(32, activation='relu'),  # Hidden layer with 64 neurons and ReLU activation
    layers.Dense(4, activation='softmax')  # Output layer with 4 neurons (for 4 classes) and softmax activation
])

# Compile the model
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Train the model and monitor performance
history = model.fit(x_train, y_train, epochs=50, batch_size=32, validation_data=(x_test, y_test))

# Evaluate the model on the test data
loss, accuracy = model.evaluate(x_test, y_test)
print(f'Accuracy on test data: {accuracy * 100:.2f}%')

# Make predictions on the test data
y_pred = np.argmax(model.predict(x_test), axis=1)

# Calculate precision, recall, and F1 score
precision = precision_score(y_test, y_pred, average='weighted')
recall = recall_score(y_test, y_pred, average='weighted')
f1 = f1_score(y_test, y_pred, average='weighted')

print(f'Precision: {precision:.4f}')
print(f'Recall: {recall:.4f}')
print(f'F1 Score: {f1:.4f}')

"""LSTM + NN"""

from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense
from keras.layers import LSTM, Dense, Dropout, Bidirectional
from keras.optimizers import Adam
from keras.regularizers import l2
from sklearn.preprocessing import MinMaxScaler
import matplotlib.pyplot as plt

train_data = pd.read_csv("")
test_data = pd.read_csv("")


train_data.drop("Direction", axis = 1, inplace = True)
test_data.drop("Direction", axis = 1, inplace = True)

x_train = train_data.drop("label", axis = 1)
x_test = test_data.drop("label", axis = 1)
y_train = train_data["label"] -1
y_test = test_data["label"] -1

scale = StandardScaler()

x_train = scale.fit_transform(x_train)
x_test = scale.fit_transform(x_test)


# Split the data into training and testing sets

# Define a neural network model
model = keras.Sequential([
    layers.Input(shape=(36,)),  # Input layer with 32 features
    layers.Dense(64, activation='relu'),  # Hidden layer with 64 neurons and ReLU activation
    layers.Dense(32, activation='relu'),  # Hidden layer with 64 neurons and ReLU activation
    layers.Dense(4, activation='softmax')  # Output layer with 4 neurons (for 4 classes) and softmax activation
])

# Compile the model
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

history = model.fit(x_train, y_train, epochs=50, batch_size=32, validation_data=(x_test, y_test))#, callbacks=[early_stopping])

# Evaluate the model on the test data
loss, accuracy = model.evaluate(x_test, y_test)
print(f'Accuracy on test data: {accuracy * 100:.2f}%')

# Predict classes on the test set
y_pred = model.predict(x_test)
y_pred_classes = np.argmax(y_pred, axis=1)


# Experimenting with different average methods
precision_weighted = precision_score(y_test, y_pred_classes, average='weighted')
recall_weighted = recall_score(y_test, y_pred_classes, average='weighted')
f1_weighted = f1_score(y_test, y_pred_classes, average='weighted')

print(f'Weighted Precision: {precision_weighted:.2f}')
print(f'Weighted Recall: {recall_weighted:.2f}')
print(f'Weighted F1 Score: {f1_weighted:.2f}')


x_pred_train_1 = model.predict(x_train)
x_pred_test_1 = model.predict(x_test)
x_pred_train_1 = pd.DataFrame(x_pred_train_1, columns = ["A", "B", "C", "D"])
x_pred_test_1 = pd.DataFrame(x_pred_test_1, columns = ["A", "B", "C", "D"])

test_data_resistance = pd.read_csv("")
train_data_resistance = pd.read_csv("")

x_train = train_data.drop("label", axis = 1)
x_test = test_data_resistance.drop("label", axis = 1)
y_train = train_data['label'] - 1
y_test = train_data_resistance["label"] - 1


x_train = scale.fit_transform(x_train)
x_test = scale.fit_transform(x_test)


model_with_regularization = Sequential()

model_with_regularization.add(LSTM(units=50, activation='relu', kernel_regularizer=l2(0.01), input_shape=(39, 16), return_sequences=True))

model_with_regularization.add(LSTM(units=50, activation='relu', kernel_regularizer=l2(0.01), return_sequences=True))
model_with_regularization.add(Dense(units=1, activation='sigmoid'))

model_with_regularization.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])


# Train the model and monitor performance
history_with_regularization = model_with_regularization.fit(x_train, y_train_2, epochs=50, batch_size=32, validation_data=(x_test, y_test_2))

# Evaluate the model on the test data
loss_with_regularization, accuracy_with_regularization = model_with_regularization.evaluate(x_test, y_test_2)
print(f'Accuracy on test data: {accuracy_with_regularization * 100:.2f}%')